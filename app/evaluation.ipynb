{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System Evaluation with RAGAS Metrics\n",
    "\n",
    "This notebook evaluates the Julius Caesar RAG system using multiple metrics:\n",
    "- **Faithfulness**: How factually accurate is the answer based on context?\n",
    "- **Answer Relevancy**: How relevant is the answer to the question?\n",
    "- **Context Precision**: How precise is the retrieved context?\n",
    "- **Context Recall**: How much of the ground truth is captured?\n",
    "- **Answer Correctness**: Overall correctness of the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_correctness\n",
    ")\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "API_URL = \"http://127.0.0.1:8000/query\"\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"AIzaSyBIgBGOVlg-QF9yCcl2T3ObW_3kofjmlcI\")\n",
    "\n",
    "# Set environment variable for RAGAS\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "print(f\"API URL: {API_URL}\")\n",
    "print(f\"API Key configured: {'Yes' if GOOGLE_API_KEY else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset\n",
    "\n",
    "We'll create a test dataset with questions, ground truth answers, and evaluate the system's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions with ground truth answers\n",
    "test_data = [\n",
    "    {\n",
    "        \"question\": \"Who is Brutus?\",\n",
    "        \"ground_truth\": \"Brutus is a Roman senator and one of the main conspirators in the assassination of Julius Caesar. He is portrayed as an honorable man who joins the conspiracy because he believes Caesar's ambition threatens the Roman Republic.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What does the Soothsayer say to Caesar?\",\n",
    "        \"ground_truth\": \"The Soothsayer warns Caesar to 'Beware the Ides of March', which is March 15th, the day Caesar is ultimately assassinated.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Why does Brutus kill Caesar?\",\n",
    "        \"ground_truth\": \"Brutus kills Caesar because he believes Caesar's ambition would lead to tyranny and the destruction of the Roman Republic. He acts out of love for Rome rather than personal hatred for Caesar.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What happens at Caesar's funeral?\",\n",
    "        \"ground_truth\": \"At Caesar's funeral, Brutus speaks first and explains the reasons for the assassination. Then Mark Antony delivers his famous 'Friends, Romans, countrymen' speech, which turns the crowd against the conspirators.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who is Cassius?\",\n",
    "        \"ground_truth\": \"Cassius is a Roman senator and the main instigator of the conspiracy against Caesar. He manipulates Brutus into joining the plot by appealing to his sense of honor and republican ideals.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the relationship between Caesar and Brutus?\",\n",
    "        \"ground_truth\": \"Caesar trusts and loves Brutus, treating him almost like a son. This makes Brutus's betrayal particularly tragic, as evidenced by Caesar's famous last words 'Et tu, Brute?' (And you, Brutus?).\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What role does Mark Antony play?\",\n",
    "        \"ground_truth\": \"Mark Antony is Caesar's loyal friend and supporter. After Caesar's assassination, he skillfully turns public opinion against the conspirators through his funeral oration and eventually leads forces against them.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are Caesar's last words?\",\n",
    "        \"ground_truth\": \"Caesar's last words are 'Et tu, Brute? Then fall, Caesar!' expressing his shock and betrayal at seeing Brutus among his assassins.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created test dataset with {len(test_data)} questions\")\n",
    "for i, item in enumerate(test_data, 1):\n",
    "    print(f\"{i}. {item['question']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the RAG System\n",
    "\n",
    "Let's query our RAG API and collect the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the RAG system and collect results\n",
    "results = []\n",
    "\n",
    "print(\"Querying RAG system...\\\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, item in enumerate(test_data, 1):\n",
    "    question = item['question']\n",
    "    print(f\"\\\\n[{i}/{len(test_data)}] Question: {question}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Query the API\n",
    "        response = requests.post(API_URL, json={\"query\": question}, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract answer and contexts\n",
    "        answer = data.get('answer', '')\n",
    "        sources = data.get('sources', [])\n",
    "        contexts = [source['chunk'] for source in sources]\n",
    "        \n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(f\"\\\\nRetrieved {len(contexts)} context chunks\")\n",
    "        \n",
    "        # Store result\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"contexts\": contexts,\n",
    "            \"ground_truth\": item['ground_truth']\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": \"Error: Could not get response\",\n",
    "            \"contexts\": [],\n",
    "            \"ground_truth\": item['ground_truth']\n",
    "        })\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(f\"\\\\nCollected {len(results)} responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for RAGAS Evaluation\n",
    "\n",
    "RAGAS requires data in a specific format using the Hugging Face Dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to RAGAS format\n",
    "ragas_data = {\n",
    "    \"question\": [r[\"question\"] for r in results],\n",
    "    \"answer\": [r[\"answer\"] for r in results],\n",
    "    \"contexts\": [r[\"contexts\"] for r in results],\n",
    "    \"ground_truth\": [r[\"ground_truth\"] for r in results]\n",
    "}\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_dict(ragas_data)\n",
    "\n",
    "print(\"Dataset created for RAGAS evaluation\")\n",
    "print(f\"Number of samples: {len(dataset)}\")\n",
    "print(f\"\\\\nDataset columns: {dataset.column_names}\")\n",
    "print(f\"\\\\nSample data:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure RAGAS with Google Gemini\n",
    "\n",
    "We'll use Google Gemini for evaluation instead of OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LLM and Embeddings for RAGAS\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "print(\"RAGAS configured with Google Gemini\")\n",
    "print(f\"LLM Model: gemini-2.5-flash\")\n",
    "print(f\"Embedding Model: models/embedding-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run RAGAS Evaluation\n",
    "\n",
    "Now let's evaluate our RAG system using multiple metrics:\n",
    "\n",
    "1. **Faithfulness**: Measures how factually accurate the answer is based on the given context\n",
    "2. **Answer Relevancy**: Measures how relevant the answer is to the question\n",
    "3. **Context Precision**: Measures the signal-to-noise ratio of retrieved context\n",
    "4. **Context Recall**: Measures how much of the ground truth is captured in the context\n",
    "5. **Answer Correctness**: Measures the overall correctness of the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics to evaluate\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_correctness\n",
    "]\n",
    "\n",
    "print(\"Evaluating RAG system with RAGAS metrics...\")\n",
    "print(\"This may take a few minutes...\\\\n\")\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_result = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=metrics,\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "print(\"\\\\nEvaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display overall scores\n",
    "print(\"=\"*80)\n",
    "print(\"RAGAS EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\\\nOverall Scores (0-1 scale, higher is better):\\\\n\")\n",
    "\n",
    "scores = evaluation_result\n",
    "\n",
    "print(f\"1. Faithfulness:        {scores['faithfulness']:.4f}\")\n",
    "print(f\"   -> How factually accurate is the answer based on context?\")\n",
    "print()\n",
    "\n",
    "print(f\"2. Answer Relevancy:    {scores['answer_relevancy']:.4f}\")\n",
    "print(f\"   -> How relevant is the answer to the question?\")\n",
    "print()\n",
    "\n",
    "print(f\"3. Context Precision:   {scores['context_precision']:.4f}\")\n",
    "print(f\"   -> How precise is the retrieved context?\")\n",
    "print()\n",
    "\n",
    "print(f\"4. Context Recall:      {scores['context_recall']:.4f}\")\n",
    "print(f\"   -> How much ground truth is captured in context?\")\n",
    "print()\n",
    "\n",
    "print(f\"5. Answer Correctness:  {scores['answer_correctness']:.4f}\")\n",
    "print(f\"   -> Overall correctness of the answer\")\n",
    "print()\n",
    "\n",
    "# Calculate average score\n",
    "avg_score = sum([scores['faithfulness'], scores['answer_relevancy'], \n",
    "                 scores['context_precision'], scores['context_recall'], \n",
    "                 scores['answer_correctness']]) / 5\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"AVERAGE SCORE: {avg_score:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for better visualization\n",
    "results_df = evaluation_result.to_pandas()\n",
    "\n",
    "print(\"\\\\nDetailed Results per Question:\\\\n\")\n",
    "print(results_df[['question', 'faithfulness', 'answer_relevancy', \n",
    "                  'context_precision', 'context_recall', 'answer_correctness']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Interpretation\n",
    "\n",
    "### Score Ranges:\n",
    "- **0.8 - 1.0**: Excellent\n",
    "- **0.6 - 0.8**: Good\n",
    "- **0.4 - 0.6**: Fair\n",
    "- **0.0 - 0.4**: Needs Improvement\n",
    "\n",
    "### What Each Metric Means:\n",
    "\n",
    "1. **Faithfulness (0-1)**\n",
    "   - Measures if the answer contains only information from the context\n",
    "   - High score = No hallucinations\n",
    "   - Low score = Answer contains made-up information\n",
    "\n",
    "2. **Answer Relevancy (0-1)**\n",
    "   - Measures how well the answer addresses the question\n",
    "   - High score = Directly answers the question\n",
    "   - Low score = Answer is off-topic or vague\n",
    "\n",
    "3. **Context Precision (0-1)**\n",
    "   - Measures if retrieved chunks are relevant\n",
    "   - High score = All retrieved chunks are useful\n",
    "   - Low score = Many irrelevant chunks retrieved\n",
    "\n",
    "4. **Context Recall (0-1)**\n",
    "   - Measures if context contains all needed information\n",
    "   - High score = Context has everything to answer\n",
    "   - Low score = Missing important information\n",
    "\n",
    "5. **Answer Correctness (0-1)**\n",
    "   - Measures factual and semantic similarity to ground truth\n",
    "   - High score = Answer matches expected answer\n",
    "   - Low score = Answer differs from ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance categorization\n",
    "def categorize_score(score):\n",
    "    if score >= 0.8:\n",
    "        return \"Excellent\"\n",
    "    elif score >= 0.6:\n",
    "        return \"Good\"\n",
    "    elif score >= 0.4:\n",
    "        return \"Fair\"\n",
    "    else:\n",
    "        return \"Needs Improvement\"\n",
    "\n",
    "print(\"\\\\nPerformance Summary:\\\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "metrics_summary = {\n",
    "    \"Faithfulness\": scores['faithfulness'],\n",
    "    \"Answer Relevancy\": scores['answer_relevancy'],\n",
    "    \"Context Precision\": scores['context_precision'],\n",
    "    \"Context Recall\": scores['context_recall'],\n",
    "    \"Answer Correctness\": scores['answer_correctness']\n",
    "}\n",
    "\n",
    "for metric_name, score in metrics_summary.items():\n",
    "    category = categorize_score(score)\n",
    "    print(f\"{metric_name:20s}: {score:.4f} - {category}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\\\\nOverall System Performance: {categorize_score(avg_score)}\")\n",
    "print(f\"Average Score: {avg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Answers Review\n",
    "\n",
    "Let's look at a few sample Q&A pairs to understand the system's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample Q&A pairs\n",
    "print(\"\\\\nSample Question-Answer Pairs:\\\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(min(3, len(results))):\n",
    "    print(f\"\\\\n[Sample {i+1}]\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Question: {results[i]['question']}\")\n",
    "    print(f\"\\\\nGenerated Answer:\\\\n{results[i]['answer']}\")\n",
    "    print(f\"\\\\nGround Truth:\\\\n{results[i]['ground_truth']}\")\n",
    "    print(f\"\\\\nNumber of Context Chunks: {len(results[i]['contexts'])}\")\n",
    "    \n",
    "    if i < len(results_df):\n",
    "        print(f\"\\\\nScores for this question:\")\n",
    "        print(f\"  Faithfulness: {results_df.iloc[i]['faithfulness']:.4f}\")\n",
    "        print(f\"  Answer Relevancy: {results_df.iloc[i]['answer_relevancy']:.4f}\")\n",
    "        print(f\"  Answer Correctness: {results_df.iloc[i]['answer_correctness']:.4f}\")\n",
    "    \n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file\n",
    "output_file = \"evaluation_results.json\"\n",
    "\n",
    "output_data = {\n",
    "    \"overall_scores\": {\n",
    "        \"faithfulness\": float(scores['faithfulness']),\n",
    "        \"answer_relevancy\": float(scores['answer_relevancy']),\n",
    "        \"context_precision\": float(scores['context_precision']),\n",
    "        \"context_recall\": float(scores['context_recall']),\n",
    "        \"answer_correctness\": float(scores['answer_correctness']),\n",
    "        \"average_score\": float(avg_score)\n",
    "    },\n",
    "    \"detailed_results\": results_df.to_dict('records')\n",
    "}\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n",
    "\n",
    "# Also save as CSV\n",
    "csv_file = \"evaluation_results.csv\"\n",
    "results_df.to_csv(csv_file, index=False)\n",
    "print(f\"Detailed results saved to {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This evaluation provides a comprehensive assessment of the RAG system using industry-standard metrics:\n",
    "\n",
    "- **Faithfulness** ensures no hallucinations\n",
    "- **Answer Relevancy** ensures questions are properly addressed\n",
    "- **Context Precision** ensures quality retrieval\n",
    "- **Context Recall** ensures completeness\n",
    "- **Answer Correctness** ensures factual accuracy\n",
    "\n",
    "Use these metrics to:\n",
    "1. Track improvements over time\n",
    "2. Compare different retrieval strategies\n",
    "3. Identify weaknesses in the system\n",
    "4. Make data-driven optimization decisions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
